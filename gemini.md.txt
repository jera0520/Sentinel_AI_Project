# 프로젝트 학습 가이드

<aside>

### 1. 기초 이해

1. 환경 설정
    
    **Why? (최상위 폴더는 어디를 기준으로 해야 돼? )**
    
    ⇒ 모든 명령어는 프로젝트의 최상위 폴더에서 시작하는 것이 기본이며, main_scale.py 와 같은 파일이 있는 곳이 최상위 폴더이다.
    
    ```
    cd /home/jera/Sentinel_AI_Project/anu_example
    ```
    
2. 가상 환경 활성화
    
    **Why? (venv가 뭐고 왜 여기서 해야 돼? )**
    
    ⇒ venv는 독립적인 작업공간으로서 개발 시 충돌을 막기 위하여 가상 공산을 만든다.
    
    ```
    # venv폴더 안에 bin 폴더 안에 activate 가 있어서 경로를 설정해주는 것.
    source venv/bin/activate
    ```
    
3. 프로젝트 파일 분석
    
    ```
    VideoParser(원자재 처리기)
    test.mp4 파일을 프레임 단위로 분해하고, 동시에 업스케일 버전을 만든다.
    
    DetectParser(주 조립 라인)
    첫 번째 컨베이어 벨트에서 프레임을 가져와, YOLOv4, ByteTrack 사용해 사람을 찾고 추척 ID 번호를 붙인다.
    
    DispEvent(품질 검수 및 전시)
    두 번째 컨베이어 벤트에서 최종 완성품을 자겨와 화면에 표시하고 프레임에 네모 상자와 ID를 그린다.
    ```
    
4. main_scale.py 테스트 진행
    
    ```
     python3 main_scale.py
    ```
    

### 2. 데이터 정제

1. 환경 설정
    
    **Why? (최상위 폴더는 어디를 기준으로 해야 돼? )**
    
    ⇒ 모든 명령어는 프로젝트의 최상위 폴더에서 시작하는 것이 기본이며, main_scale.py 와 같은 파일이 있는 곳이 최상위 폴더이다.
    
    ```
    cd /home/jera/Sentinel_AI_Project/anu_example
    ```
    
2. 가상 환경 활성화
    
    **Why? (venv가 뭐고 왜 여기서 해야 돼? )**
    
    ⇒ venv는 독립적인 작업공간으로서 개발 시 충돌을 막기 위하여 가상 공산을 만든다.
    
    ```
    # venv폴더 안에 bin 폴더 안에 activate 가 있어서 경로를 설정해주는 것.
    source venv/bin/activate
    ```
    
3. 프로젝트 파일 분석
    
    ```
    VideoParser(원자재 처리기)
    test.mp4 파일을 프레임 단위로 분해하고, 동시에 업스케일 버전을 만든다.
    
    DetectParser(주 조립 라인)
    첫 번째 컨베이어 벨트에서 프레임을 가져와, YOLOv4, ByteTrack 사용해 사람을 찾고 추척 ID 번호를 붙인다.
    
    DispEvent(품질 검수 및 전시)
    두 번째 컨베이어 벤트에서 최종 완성품을 자겨와 화면에 표시하고 프레임에 네모 상자와 ID를 그린다.
    ```
    
4. main_scale.py 테스트 진행
    
    ```
     python3 main_scale.py
    ```
    
5. 데이터 수집
    
    **Why? (뭘 수집해야 하고 무슨 기준으로 수집하는 거야? )**
    
    ⇒ 1. 프로젝트 구조를 파악한 후 학습에 필요한 클래스 확인
    
         2. AI_hub 데이터 설명 읽은 후 선택해서 다운로드
    
    ```
    공사현장 안전장비 인식 이미지
    ```
    
6. 데이터 정제
    
    **Why? (데이터 정제란 뭘 하는 과정이고 이걸 왜 해야 하는 거야?)**
    
    ⇒ 데이터 정제란? 학습에 필요한 데이터만 남겨두고 정리하는 것으로 **‘안전모’, ‘쓰러짐’**이 명확하게 보이는 사진만 남겨두는 과정
    
    특히, darknet 프레임워크는 json 파일이 아닌 txt 파일로 정제하는 과정이 필요함.
    
7. json → txt 파일로 전환 (yolo_labels)
    
    ```
    # 키포인트 Json 파일을 Yolo 바운딩 박스 TXT로 변환하는 스크립트
    1. convert_json_to_yolo.py( 메인 변환 프로그램 )
    
    # 단일 파일로만 테스트하는 스크립트
    2. test_converter.py
    
    # 200개만 바꾸는 스크립트
    3. run_conversion.sh
    
    # 생성된 데이터
    TXT 파일: 249개 (YOLO 어노테이션)
    JPG 파일: 200개 (이미지)
    classes.names: 클래스 정의 파일
    
    # YOLO 라벨 예시
            97 +  2 0.434115 0.367593 0.255625 0.260000  # fallen_person
            98 +  0 0.517448 0.327407 0.074167 0.238519  # helmet
    ```
    
8. YOLO Mark (데이터 정제)
    
    **Why_1? (YOLO Mark란? )**
    
    ⇒ YOLO 학습용 바운딩 박스 어노테이션 GUI 도구이다.
    
    **Why_2? (Framework Auto 라벨링이란? )**
    
    ⇒ 기존 훈련된 모델로 1차 자동 라벨링 하는 것.
    
    ```
    🛠️ 필요한 것들
    
      1. CMake: 프로그램 빌드 도구
      2. OpenCV: 이미지 처리 라이브러리
      3. YOLO Mark 소스코드
      
    # 필수 도구 설치
      sudo apt-get install -y cmake build-essential
      
    # YOLO Mark (Github에서) 설치
    cd /home/jera/Sentinel_AI_Project/anu_example && git clone https://github.com/AlexeyAB/Yolo_mark.git
    
    # Yolo_mark 폴더로 이동
      cd Yolo_mark
    
    ```
    
9. YOLO MARK 빌드(컴파일)
    
    ```
    # 빌드 폴더 생성
      mkdir -p build cd build
      
    # CMake로 빌드 준비
      cmake ..
      
    # 컴파일 실행
    	make
    ```
    
10. obj 파일 생성
    
    ```
    📁 디렉토리 구조 준비
    
      YOLO Mark는 특정 폴더 구조를 기대해요:
    
      Yolo_mark/
      ├── x64/
      │   └── Release/
      │       ├── data/
      │       │   ├── img/          ← 이미지 파일들 (JPG)
      │       │   └── obj.data      ← 설정 파일
      │       └── obj.names         ← 클래스 이름
      
      
      # 필요한 폴더 생성 
      mkdir -p x64/Release/data/img
      
      # obj.names 파일 생성(아래 명령어는 메모장 기능)
      cat > x64/Release/obj.names << 'EOF'
      helmet
      person
      fallen_person
      EOF
      
      # obj.data 파일 생성
      cat > x64/Release/data/obj.data << 'EOF'
      classes = 3
      names = obj.names
      EOF
    ```
    
11. 이미지와 라벨 파일 복사
    
    ```
    방법 2: 직접 복사
      # 이미지 파일 복사
      cp /home/jera/Sentinel_AI_Project/anu_example/yolo_labels/*.jpg x64/Release/data/img/
    
      # TXT 파일 복사
      cp /home/jera/Sentinel_AI_Project/anu_example/yolo_labels/*.txt x64/Release/data/img/
    ```
    

커스텀 모델 학습

1. obj.names 파일 생성
    
    ```
    # darknet 폴더로 이동
    cd ~/Sentinel_AI_Project/darknet
    
    # obj.names 파일 생성
    cat > data/obj.names << EOF
    helmet
    person
    fallen
    EOF
    ```
    
2. `train.txt` & `valid.txt` 파일 생성
    
    ```
    # 1. 학습용 이미지 경로 목록 생성
    find /home/jera/Sentinel_AI_Project/custom_dataset/images -name "*.jpg" > data/train.txt
    
    # 2. 검증용 이미지 경로 목록 생성 (지금은 데이터가 적으니, 학습용과 동일하게 사용)
    cp data/train.txt data/valid.txt
    ```
    
3. `obj.data` 파일 생성
    
    ```
    # darknet 폴더 안에서 실행
    cp cfg/yolov4.cfg cfg/yolov4-custom.cfg
    ```
    

YOLOv4 모델 구조 파일 수정하기

1. 설정 파일 복사(원본 `yolov4.cfg`는 그대로 두고, 우리 프로젝트 전용 설정 파일을 만들기)
    
    ```
    # 1. 학습용 이미지 경로 목록 생성
    find /home/jera/Sentinel_AI_Project/custom_dataset/images -name "*.jpg" > data/train.txt
    
    # 2. 검증용 이미지 경로 목록 생성 (지금은 데이터가 적으니, 학습용과 동일하게 사용)
    cp data/train.txt data/valid.txt
    ```
    
2. `yolov4-custom.cfg` 파일 수정
    
    ```
    batch=64
    
    subdivisions=16
    
    max_batches=6000 (클래스 개수 * 2000 공식을 따르며, 최소 6000으로 설정)
    
    steps=4800, 5400 (max_batches의 80%, 90% 값)
    
    파일 전체에서 classes=80 으로 된 부분을 찾아 모두 classes=3 (우리가 설정한 클래스 개수)으로 변경. (총 3곳)
    
    classes 바로 위에 있는 [convolutional] 레이어의 filters 값을 filters=24 ((클래스 개수 + 5) * 3 공식) 로 변경. (총 3곳)
    ```
    

학습 시작하기

1. **사전 훈련된 가중치 다운로드**:
    
    ```
    # darknet 폴더 안에서 실행
    wget https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.conv.137
    ```
    
2. 학습 시작
    
    ```
    ./darknet detector train data/obj.data cfg/yolov4-custom.cfg yolov4.conv.137 -dont_show -map
    ```
    
3. 오류 발생
    
    ```
     mAP(성능)를 계산하는 과정에서 발생, -map 옵션을 사용하여 학습 중 mAP 계산을 활성화함. 이 기능이 특정 cuDNN 버전과 충돌하면서 CUDNN_STATUS_BAD_PARAM 오류 발생.
    
    # 해결방법
    일반적으로 설정 파일(yolov4-custom.cfg)을 수정하여 해결함.
    
    cfg/yolov4-custom.cfg 파일의 맨 위 [net] 섹션에 cudnn_benchmark=0 한 줄을 추가하여 cuDNN의 알고리즘 선택 방식을 보다 안정적인 모드로 변경
    ```
    

검출 결과 테스트(성능 확인)

1. 테스트 명령어 실행(test_image.jpg는 내 파일 이름으로 바꾸기)
    
    ```
    ./darknet detector test data/obj.data cfg/yolov4-custom.cfg backup/yolov4-custom_best.weights /path/to/your/**test_image.jpg**
    ```
    
</aside>

Top Down 방식 선호하심

기준을 나눈 다음에 스코어링 방식으로 해서 안전잔구 미착용 알고리즘 

앞에는 YOlo v4로 해서 사람을 먼저 검출.(배경과 객체 분리) 분리된 이미지를 분류기에 넣던가 모델에 넣어서 이 사람이 헬멧을 썻는지 안 썻는지 파악하는 것

왜냐하면 환경이 크기 때문에

과제 : 이미지 크롭으로 객체 값을 이미지로 저장?