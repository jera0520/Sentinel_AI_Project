# ğŸ“ 2ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ê°€ì´ë“œ (ë¹„ì „ê³µììš©)

> **ëª©í‘œ**: ì‚¬ëŒì„ ë¨¼ì € íƒì§€í•˜ê³ , ê·¸ ì‚¬ëŒì´ í—¬ë©§ì„ ì°©ìš©í–ˆëŠ”ì§€ ë¶„ë¥˜í•˜ëŠ” 2ë‹¨ê³„ ì‹œìŠ¤í…œ ë§Œë“¤ê¸°

---

## ğŸ“– ëª©ì°¨
1. [ì™œ 2ë‹¨ê³„ë¡œ ë‚˜ëˆ„ë‚˜ìš”?](#1-ì™œ-2ë‹¨ê³„ë¡œ-ë‚˜ëˆ„ë‚˜ìš”)
2. [ì „ì²´ íë¦„ ì´í•´í•˜ê¸°](#2-ì „ì²´-íë¦„-ì´í•´í•˜ê¸°)
3. [Step 1: ë°ì´í„° í¬ë¡­í•˜ê¸°](#step-1-ë°ì´í„°-í¬ë¡­í•˜ê¸°)
4. [Step 2: í—¬ë©§ ë¶„ë¥˜ê¸° í•™ìŠµí•˜ê¸°](#step-2-í—¬ë©§-ë¶„ë¥˜ê¸°-í•™ìŠµí•˜ê¸°)
5. [Step 3: íŒŒì´í”„ë¼ì¸ í†µí•©í•˜ê¸°](#step-3-íŒŒì´í”„ë¼ì¸-í†µí•©í•˜ê¸°)
6. [ë¬¸ì œ í•´ê²°](#ë¬¸ì œ-í•´ê²°)

---

## 1. ì™œ 2ë‹¨ê³„ë¡œ ë‚˜ëˆ„ë‚˜ìš”?

### í˜„ì¬ ë°©ì‹ (Single-Stage)
```
ì¹´ë©”ë¼ ì˜ìƒ â†’ YOLO â†’ person, helmet, no_helmet, fallen ë™ì‹œ íƒì§€
```
**ë¬¸ì œì :**
- ë°°ê²½ì´ ë³µì¡í•˜ë©´ ì‘ì€ í—¬ë©§ì„ ë†“ì¹¨
- í—¬ë©§ë§Œ ìˆê³  ì‚¬ëŒì´ ì—†ëŠ” ê²½ìš° ì˜¤íƒì§€
- ì •í™•ë„ê°€ ë‚®ìŒ

### ê°œì„  ë°©ì‹ (Two-Stage) â­
```
ì¹´ë©”ë¼ ì˜ìƒ â†’ [1ë‹¨ê³„] ì‚¬ëŒ íƒì§€ â†’ ì‚¬ëŒ ì˜ì—­ í¬ë¡­ 
            â†’ [2ë‹¨ê³„] í—¬ë©§ ë¶„ë¥˜ â†’ helmet / no_helmet
```
**ì¥ì :**
- âœ… ë°°ê²½ ì œê±°ë¡œ ì •í™•ë„ í–¥ìƒ
- âœ… í—¬ë©§ë§Œ ìˆëŠ” ì˜¤íƒì§€ ì œê±°
- âœ… ì‘ì€ í—¬ë©§ë„ ì˜ íƒì§€ (í¬ë¡­ í›„ í™•ëŒ€ë˜ë¯€ë¡œ)

---

## 2. ì „ì²´ íë¦„ ì´í•´í•˜ê¸°

### ğŸ¯ ì „ì²´ ì‘ì—… ë‹¨ê³„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 1: ë°ì´í„° ì¤€ë¹„ (1-2ì‹œê°„)                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1-1. person bbox ì¶”ì¶œ                               â”‚
â”‚ 1-2. ì´ë¯¸ì§€ í¬ë¡­                                     â”‚
â”‚ 1-3. helmet/no_helmet ë¼ë²¨ë§                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 2: í—¬ë©§ ë¶„ë¥˜ê¸° í•™ìŠµ (2-3ì‹œê°„)                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2-1. TensorFlow í™˜ê²½ ì„¤ì •                           â”‚
â”‚ 2-2. ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ (MobileNet)                     â”‚
â”‚ 2-3. ëª¨ë¸ í…ŒìŠ¤íŠ¸                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 3: íŒŒì´í”„ë¼ì¸ í†µí•© (1ì‹œê°„)                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 3-1. main_scale.py ìˆ˜ì •                             â”‚
â”‚ 3-2. í…ŒìŠ¤íŠ¸ ë° ê²€ì¦                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ì´ ì˜ˆìƒ ì‹œê°„: 4-6ì‹œê°„**

---

## Step 1: ë°ì´í„° í¬ë¡­í•˜ê¸°

### ğŸ“‚ ì‘ì—… í´ë” ì¤€ë¹„

**1-1. í„°ë¯¸ë„ ì—´ê¸°**
```bash
cd /home/jera/Sentinel_AI_Project
```

**1-2. ì‘ì—… í´ë” ë§Œë“¤ê¸°**
```bash
mkdir -p two_stage_data
cd two_stage_data
mkdir -p person_crops/helmet
mkdir -p person_crops/no_helmet
mkdir -p person_crops/unlabeled
```

**í´ë” êµ¬ì¡°:**
```
two_stage_data/
â””â”€â”€ person_crops/
    â”œâ”€â”€ helmet/          # í—¬ë©§ ì°©ìš©í•œ ì‚¬ëŒë“¤
    â”œâ”€â”€ no_helmet/       # í—¬ë©§ ë¯¸ì°©ìš© ì‚¬ëŒë“¤
    â””â”€â”€ unlabeled/       # ì•„ì§ ë¼ë²¨ ì•ˆ ë¶™ì¸ ê²ƒë“¤
```

---

### ğŸ“ í¬ë¡­ ìŠ¤í¬ë¦½íŠ¸ ë§Œë“¤ê¸°

**1-3. í¬ë¡­ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±**

í„°ë¯¸ë„ì— ë‹¤ìŒ ëª…ë ¹ì–´ ì…ë ¥:
```bash
cat > crop_persons.py << 'EOF'
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AI Hub ë°ì´í„°ì—ì„œ person bboxë¥¼ í¬ë¡­í•˜ì—¬ ì €ì¥í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
"""
import cv2
import os
import json
from pathlib import Path

def read_yolo_label(txt_path):
    """YOLO í˜•ì‹ì˜ txt íŒŒì¼ì—ì„œ bbox ì½ê¸°"""
    bboxes = []
    try:
        with open(txt_path, 'r') as f:
            for line in f:
                parts = line.strip().split()
                if len(parts) >= 5:
                    class_id = int(parts[0])
                    x_center = float(parts[1])
                    y_center = float(parts[2])
                    width = float(parts[3])
                    height = float(parts[4])
                    bboxes.append({
                        'class_id': class_id,
                        'x_center': x_center,
                        'y_center': y_center,
                        'width': width,
                        'height': height
                    })
    except Exception as e:
        print(f"ë¼ë²¨ ì½ê¸° ì˜¤ë¥˜: {txt_path}, {e}")
    return bboxes

def yolo_to_pixel(bbox, img_width, img_height):
    """YOLO ì¢Œí‘œë¥¼ í”½ì…€ ì¢Œí‘œë¡œ ë³€í™˜"""
    x_center = bbox['x_center'] * img_width
    y_center = bbox['y_center'] * img_height
    w = bbox['width'] * img_width
    h = bbox['height'] * img_height
    
    xmin = int(x_center - w/2)
    ymin = int(y_center - h/2)
    xmax = int(x_center + w/2)
    ymax = int(y_center + h/2)
    
    return xmin, ymin, xmax, ymax

def crop_persons(data_dir, output_dir, person_class_id=0, min_size=50):
    """
    ë°ì´í„° ë””ë ‰í† ë¦¬ì—ì„œ person í´ë˜ìŠ¤ë§Œ í¬ë¡­í•˜ì—¬ ì €ì¥
    
    Args:
        data_dir: ì´ë¯¸ì§€ì™€ ë¼ë²¨ì´ ìˆëŠ” í´ë”
        output_dir: í¬ë¡­ëœ ì´ë¯¸ì§€ë¥¼ ì €ì¥í•  í´ë”
        person_class_id: person í´ë˜ìŠ¤ ID (ê¸°ë³¸: 0)
        min_size: ìµœì†Œ í¬ê¸° (í”½ì…€, ì´ê²ƒë³´ë‹¤ ì‘ìœ¼ë©´ ë¬´ì‹œ)
    """
    data_path = Path(data_dir)
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # ì´ë¯¸ì§€ íŒŒì¼ ì°¾ê¸°
    image_files = list(data_path.glob('*.jpg')) + list(data_path.glob('*.png'))
    print(f"âœ… ì´ {len(image_files)}ê°œ ì´ë¯¸ì§€ ë°œê²¬")
    
    crop_count = 0
    skip_count = 0
    
    for img_path in image_files:
        # ë¼ë²¨ íŒŒì¼ ê²½ë¡œ
        txt_path = img_path.with_suffix('.txt')
        if not txt_path.exists():
            print(f"âš ï¸  ë¼ë²¨ ì—†ìŒ: {img_path.name}")
            continue
        
        # ì´ë¯¸ì§€ ì½ê¸°
        img = cv2.imread(str(img_path))
        if img is None:
            print(f"âŒ ì´ë¯¸ì§€ ì½ê¸° ì‹¤íŒ¨: {img_path.name}")
            continue
        
        img_height, img_width = img.shape[:2]
        
        # ë¼ë²¨ ì½ê¸°
        bboxes = read_yolo_label(txt_path)
        
        # person í´ë˜ìŠ¤ë§Œ í•„í„°ë§
        person_bboxes = [b for b in bboxes if b['class_id'] == person_class_id]
        
        if not person_bboxes:
            continue
        
        # ê° person bbox í¬ë¡­
        for idx, bbox in enumerate(person_bboxes):
            xmin, ymin, xmax, ymax = yolo_to_pixel(bbox, img_width, img_height)
            
            # ê²½ê³„ ì²´í¬
            xmin = max(0, xmin)
            ymin = max(0, ymin)
            xmax = min(img_width, xmax)
            ymax = min(img_height, ymax)
            
            # í¬ê¸° ì²´í¬
            w = xmax - xmin
            h = ymax - ymin
            if w < min_size or h < min_size:
                skip_count += 1
                continue
            
            # í¬ë¡­
            cropped = img[ymin:ymax, xmin:xmax]
            
            # ì €ì¥
            filename = f"{img_path.stem}_person{idx}.jpg"
            output_file = output_path / filename
            cv2.imwrite(str(output_file), cropped)
            crop_count += 1
            
            if crop_count % 100 == 0:
                print(f"ì§„í–‰ ì¤‘... {crop_count}ê°œ í¬ë¡­ ì™„ë£Œ")
    
    print(f"\nâœ… í¬ë¡­ ì™„ë£Œ!")
    print(f"   - ì €ì¥ëœ ì´ë¯¸ì§€: {crop_count}ê°œ")
    print(f"   - ê±´ë„ˆë›´ ì´ë¯¸ì§€: {skip_count}ê°œ (ë„ˆë¬´ ì‘ìŒ)")
    print(f"   - ì €ì¥ ìœ„ì¹˜: {output_dir}")

if __name__ == '__main__':
    # ì„¤ì •
    DATA_DIR = '/home/jera/Sentinel_AI_Project/Dataset/Data/Keypoint/1.Tranining/labels'
    OUTPUT_DIR = '/home/jera/Sentinel_AI_Project/two_stage_data/person_crops/unlabeled'
    PERSON_CLASS_ID = 0  # obj.namesì—ì„œ personì˜ ì¸ë±ìŠ¤ í™•ì¸
    
    print("=" * 60)
    print("ğŸš€ Person í¬ë¡­ ìŠ¤í¬ë¦½íŠ¸ ì‹œì‘")
    print("=" * 60)
    print(f"ğŸ“‚ ì…ë ¥ í´ë”: {DATA_DIR}")
    print(f"ğŸ’¾ ì¶œë ¥ í´ë”: {OUTPUT_DIR}")
    print(f"ğŸ‘¤ Person í´ë˜ìŠ¤ ID: {PERSON_CLASS_ID}")
    print("=" * 60)
    
    crop_persons(DATA_DIR, OUTPUT_DIR, PERSON_CLASS_ID, min_size=50)
    
    print("\nğŸ“‹ ë‹¤ìŒ ë‹¨ê³„:")
    print("1. unlabeled í´ë”ì˜ ì´ë¯¸ì§€ë“¤ì„ í™•ì¸í•˜ì„¸ìš”")
    print("2. í—¬ë©§ ì°©ìš© ì—¬ë¶€ì— ë”°ë¼ helmet / no_helmet í´ë”ë¡œ ì´ë™í•˜ì„¸ìš”")
    print("   - í—¬ë©§ O: helmet í´ë”ë¡œ")
    print("   - í—¬ë©§ X: no_helmet í´ë”ë¡œ")
EOF

chmod +x crop_persons.py
```

**1-4. ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰**
```bash
python3 crop_persons.py
```

**ì˜ˆìƒ ì¶œë ¥:**
```
============================================================
ğŸš€ Person í¬ë¡­ ìŠ¤í¬ë¦½íŠ¸ ì‹œì‘
============================================================
ğŸ“‚ ì…ë ¥ í´ë”: .../1.Tranining/labels
ğŸ’¾ ì¶œë ¥ í´ë”: .../person_crops/unlabeled
ğŸ‘¤ Person í´ë˜ìŠ¤ ID: 0
============================================================
âœ… ì´ 23899ê°œ ì´ë¯¸ì§€ ë°œê²¬
ì§„í–‰ ì¤‘... 100ê°œ í¬ë¡­ ì™„ë£Œ
ì§„í–‰ ì¤‘... 200ê°œ í¬ë¡­ ì™„ë£Œ
...
âœ… í¬ë¡­ ì™„ë£Œ!
   - ì €ì¥ëœ ì´ë¯¸ì§€: 1523ê°œ
   - ê±´ë„ˆë›´ ì´ë¯¸ì§€: 245ê°œ (ë„ˆë¬´ ì‘ìŒ)
```

---

### ğŸ·ï¸ ë¼ë²¨ë§ í•˜ê¸° (ì¤‘ìš”!)

ì´ì œ í¬ë¡­ëœ ì´ë¯¸ì§€ë¥¼ helmet / no_helmet í´ë”ë¡œ ë¶„ë¥˜í•´ì•¼ í•©ë‹ˆë‹¤.

**ë°©ë²• 1: ìˆ˜ë™ ë¶„ë¥˜ (ì¶”ì²œ, ì •í™•í•¨)**

1. íŒŒì¼ íƒìƒ‰ê¸°ë¡œ `two_stage_data/person_crops/unlabeled` í´ë” ì—´ê¸°
2. ì´ë¯¸ì§€ë¥¼ í•˜ë‚˜ì”© ë³´ë©´ì„œ:
   - í—¬ë©§ ì°©ìš© â†’ `helmet` í´ë”ë¡œ ë“œë˜ê·¸
   - í—¬ë©§ ë¯¸ì°©ìš© â†’ `no_helmet` í´ë”ë¡œ ë“œë˜ê·¸

**íŒ:**
- í•œ ë²ˆì— 100ê°œì”© ë‚˜ëˆ ì„œ ì‘ì—…í•˜ì„¸ìš” (ì§€ì¹˜ì§€ ì•Šê²Œ)
- í™•ì‹¤í•˜ì§€ ì•Šì€ ì´ë¯¸ì§€ëŠ” ê±´ë„ˆë›°ì„¸ìš”
- ê° í´ë˜ìŠ¤ë³„ ìµœì†Œ 100ê°œì”©ì€ í•„ìš”í•©ë‹ˆë‹¤

**ë°©ë²• 2: ìŠ¤í¬ë¦½íŠ¸ë¡œ ìë™ ë¶„ë¥˜ (ë¹ ë¥´ì§€ë§Œ ë¶€ì •í™•)**

```bash
# AI Hub ë°ì´í„°ì˜ ì›ë³¸ JSONì„ ì½ì–´ì„œ ìë™ ë¶„ë¥˜í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
# (JSON íŒŒì¼ì´ ìˆëŠ” ê²½ìš°ì—ë§Œ ê°€ëŠ¥)
```

---

### âœ… ë°ì´í„° í™•ì¸

ë¼ë²¨ë§ì´ ëë‚˜ë©´ í™•ì¸:
```bash
cd /home/jera/Sentinel_AI_Project/two_stage_data/person_crops

# ê° í´ë”ì˜ ì´ë¯¸ì§€ ê°œìˆ˜ í™•ì¸
echo "helmet: $(ls helmet/*.jpg 2>/dev/null | wc -l)ê°œ"
echo "no_helmet: $(ls no_helmet/*.jpg 2>/dev/null | wc -l)ê°œ"
echo "unlabeled: $(ls unlabeled/*.jpg 2>/dev/null | wc -l)ê°œ"
```

**ëª©í‘œ:**
- helmet: ìµœì†Œ 100ê°œ (ë” ë§ì„ìˆ˜ë¡ ì¢‹ìŒ)
- no_helmet: ìµœì†Œ 100ê°œ
- unlabeled: 0ê°œ (ëª¨ë‘ ë¶„ë¥˜ ì™„ë£Œ)

---

## Step 2: í—¬ë©§ ë¶„ë¥˜ê¸° í•™ìŠµí•˜ê¸°

### ğŸ”§ TensorFlow ì„¤ì¹˜

**2-1. ê°€ìƒí™˜ê²½ ë§Œë“¤ê¸°**
```bash
cd /home/jera/Sentinel_AI_Project/two_stage_data
python3 -m venv classifier_env
source classifier_env/bin/activate
```

**2-2. í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜**
```bash
pip install --upgrade pip
pip install tensorflow==2.10.0
pip install opencv-python
pip install numpy
pip install matplotlib
pip install pillow
```

ì„¤ì¹˜ ì‹œê°„: ì•½ 5-10ë¶„

---

### ğŸ¤– ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸

**2-3. í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±**

```bash
cat > train_helmet_classifier.py << 'EOF'
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
í—¬ë©§ ì°©ìš© ì—¬ë¶€ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
"""
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import matplotlib.pyplot as plt
import os

print("=" * 60)
print("ğŸ¤– í—¬ë©§ ë¶„ë¥˜ê¸° í•™ìŠµ ì‹œì‘")
print("=" * 60)
print(f"TensorFlow ë²„ì „: {tf.__version__}")
print(f"GPU ì‚¬ìš© ê°€ëŠ¥: {tf.test.is_gpu_available()}")
print("=" * 60)

# ========== ì„¤ì • ==========
DATA_DIR = 'person_crops'  # helmet, no_helmet í´ë”ê°€ ìˆëŠ” ìœ„ì¹˜
IMG_SIZE = (224, 224)  # ì´ë¯¸ì§€ í¬ê¸°
BATCH_SIZE = 32
EPOCHS = 20  # í•™ìŠµ ë°˜ë³µ íšŸìˆ˜
LEARNING_RATE = 0.001

# ========== ë°ì´í„° ë¡œë“œ ==========
print("\nğŸ“‚ ë°ì´í„° ë¡œë”© ì¤‘...")

# ë°ì´í„° ì¦ê°• (ë°ì´í„°ë¥¼ ë‹¤ì–‘í•˜ê²Œ ë³€í˜•í•˜ì—¬ ë” ì˜ í•™ìŠµí•˜ê²Œ í•¨)
train_datagen = ImageDataGenerator(
    rescale=1./255,  # 0-255 ê°’ì„ 0-1ë¡œ ì •ê·œí™”
    rotation_range=20,  # ëœë¤ íšŒì „
    width_shift_range=0.2,  # ì¢Œìš° ì´ë™
    height_shift_range=0.2,  # ìƒí•˜ ì´ë™
    horizontal_flip=True,  # ì¢Œìš° ë°˜ì „
    validation_split=0.2  # 20%ëŠ” ê²€ì¦ìš©ìœ¼ë¡œ
)

# í•™ìŠµ ë°ì´í„°
train_generator = train_datagen.flow_from_directory(
    DATA_DIR,
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='binary',  # helmet(0) vs no_helmet(1)
    subset='training'
)

# ê²€ì¦ ë°ì´í„°
validation_generator = train_datagen.flow_from_directory(
    DATA_DIR,
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='binary',
    subset='validation'
)

print(f"âœ… í•™ìŠµ ë°ì´í„°: {train_generator.samples}ê°œ")
print(f"âœ… ê²€ì¦ ë°ì´í„°: {validation_generator.samples}ê°œ")
print(f"ğŸ“‹ í´ë˜ìŠ¤: {train_generator.class_indices}")

# ========== ëª¨ë¸ êµ¬ì¶• ==========
print("\nğŸ—ï¸  ëª¨ë¸ êµ¬ì¶• ì¤‘...")

# MobileNetV2 ì‚¬ìš© (ê°€ë³ê³  ë¹ ë¥¸ ëª¨ë¸)
base_model = keras.applications.MobileNetV2(
    input_shape=(224, 224, 3),
    include_top=False,  # ë§ˆì§€ë§‰ ë¶„ë¥˜ ë ˆì´ì–´ ì œì™¸
    weights='imagenet'  # ImageNetìœ¼ë¡œ ì‚¬ì „ í•™ìŠµëœ ê°€ì¤‘ì¹˜ ì‚¬ìš©
)

# ì‚¬ì „ í•™ìŠµëœ ë¶€ë¶„ì€ ê³ ì • (ì²˜ìŒì—ëŠ”)
base_model.trainable = False

# ìš°ë¦¬ ëª¨ë¸ êµ¬ì¶•
model = keras.Sequential([
    base_model,
    layers.GlobalAveragePooling2D(),
    layers.Dropout(0.2),  # ê³¼ì í•© ë°©ì§€
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.2),
    layers.Dense(1, activation='sigmoid')  # 0 or 1 ì¶œë ¥
])

# ëª¨ë¸ ì»´íŒŒì¼
model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),
    loss='binary_crossentropy',
    metrics=['accuracy']
)

model.summary()

# ========== ëª¨ë¸ í•™ìŠµ ==========
print("\nğŸ”¥ í•™ìŠµ ì‹œì‘...")

# í•™ìŠµ ì¤‘ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
checkpoint = keras.callbacks.ModelCheckpoint(
    'helmet_classifier_best.h5',
    monitor='val_accuracy',
    save_best_only=True,
    verbose=1
)

# í•™ìŠµ
history = model.fit(
    train_generator,
    epochs=EPOCHS,
    validation_data=validation_generator,
    callbacks=[checkpoint]
)

# ========== ê²°ê³¼ ì €ì¥ ==========
print("\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...")
model.save('helmet_classifier_final.h5')
print("âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: helmet_classifier_final.h5")

# ========== í•™ìŠµ ê·¸ë˜í”„ ==========
print("\nğŸ“Š í•™ìŠµ ê·¸ë˜í”„ ìƒì„± ì¤‘...")

plt.figure(figsize=(12, 4))

# ì •í™•ë„ ê·¸ë˜í”„
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)

# ì†ì‹¤ ê·¸ë˜í”„
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.savefig('training_history.png')
print("âœ… ê·¸ë˜í”„ ì €ì¥ ì™„ë£Œ: training_history.png")

# ========== ìµœì¢… ì„±ëŠ¥ ì¶œë ¥ ==========
print("\n" + "=" * 60)
print("ğŸ‰ í•™ìŠµ ì™„ë£Œ!")
print("=" * 60)
print(f"ğŸ“Š ìµœì¢… í•™ìŠµ ì •í™•ë„: {history.history['accuracy'][-1]:.2%}")
print(f"ğŸ“Š ìµœì¢… ê²€ì¦ ì •í™•ë„: {history.history['val_accuracy'][-1]:.2%}")
print(f"ğŸ’¾ ì €ì¥ëœ íŒŒì¼:")
print(f"   - helmet_classifier_best.h5 (ìµœê³  ì„±ëŠ¥ ëª¨ë¸)")
print(f"   - helmet_classifier_final.h5 (ìµœì¢… ëª¨ë¸)")
print(f"   - training_history.png (í•™ìŠµ ê·¸ë˜í”„)")
print("=" * 60)
EOF

chmod +x train_helmet_classifier.py
```

**2-4. í•™ìŠµ ì‹¤í–‰**
```bash
source classifier_env/bin/activate
python3 train_helmet_classifier.py
```

**ì˜ˆìƒ ì†Œìš” ì‹œê°„:**
- GPU ìˆìŒ: 5-10ë¶„
- GPU ì—†ìŒ: 20-30ë¶„

**í•™ìŠµ ê³¼ì • ì¶œë ¥ ì˜ˆì‹œ:**
```
============================================================
ğŸ¤– í—¬ë©§ ë¶„ë¥˜ê¸° í•™ìŠµ ì‹œì‘
============================================================
TensorFlow ë²„ì „: 2.10.0
GPU ì‚¬ìš© ê°€ëŠ¥: True
============================================================

ğŸ“‚ ë°ì´í„° ë¡œë”© ì¤‘...
âœ… í•™ìŠµ ë°ì´í„°: 240ê°œ
âœ… ê²€ì¦ ë°ì´í„°: 60ê°œ
ğŸ“‹ í´ë˜ìŠ¤: {'helmet': 0, 'no_helmet': 1}

ğŸ—ï¸  ëª¨ë¸ êµ¬ì¶• ì¤‘...
Model: "sequential"
...

ğŸ”¥ í•™ìŠµ ì‹œì‘...
Epoch 1/20
8/8 [==============================] - 12s 1s/step - loss: 0.6234 - accuracy: 0.6917 - val_loss: 0.4521 - val_accuracy: 0.8000
Epoch 2/20
8/8 [==============================] - 10s 1s/step - loss: 0.4123 - accuracy: 0.8250 - val_loss: 0.3012 - val_accuracy: 0.8833
...
Epoch 20/20
8/8 [==============================] - 10s 1s/step - loss: 0.1234 - accuracy: 0.9583 - val_loss: 0.1521 - val_accuracy: 0.9333

ğŸ‰ í•™ìŠµ ì™„ë£Œ!
ğŸ“Š ìµœì¢… í•™ìŠµ ì •í™•ë„: 95.83%
ğŸ“Š ìµœì¢… ê²€ì¦ ì •í™•ë„: 93.33%
```

---

### ğŸ§ª ëª¨ë¸ í…ŒìŠ¤íŠ¸

**2-5. í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±**

```bash
cat > test_classifier.py << 'EOF'
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
í•™ìŠµëœ í—¬ë©§ ë¶„ë¥˜ê¸° í…ŒìŠ¤íŠ¸
"""
import tensorflow as tf
from tensorflow import keras
import cv2
import numpy as np
import sys

# ëª¨ë¸ ë¡œë“œ
print("ğŸ¤– ëª¨ë¸ ë¡œë”© ì¤‘...")
model = keras.models.load_model('helmet_classifier_best.h5')
print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

def predict_image(image_path):
    """ì´ë¯¸ì§€ì—ì„œ í—¬ë©§ ì°©ìš© ì—¬ë¶€ ì˜ˆì¸¡"""
    # ì´ë¯¸ì§€ ì½ê¸°
    img = cv2.imread(image_path)
    if img is None:
        print(f"âŒ ì´ë¯¸ì§€ ì½ê¸° ì‹¤íŒ¨: {image_path}")
        return None
    
    # ì „ì²˜ë¦¬
    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    img_resized = cv2.resize(img_rgb, (224, 224))
    img_normalized = img_resized / 255.0
    img_batch = np.expand_dims(img_normalized, axis=0)
    
    # ì˜ˆì¸¡
    prediction = model.predict(img_batch, verbose=0)[0][0]
    
    # ê²°ê³¼ í•´ì„
    if prediction < 0.5:
        label = "helmet"
        confidence = (1 - prediction) * 100
    else:
        label = "no_helmet"
        confidence = prediction * 100
    
    return label, confidence

if __name__ == '__main__':
    if len(sys.argv) < 2:
        print("ì‚¬ìš©ë²•: python3 test_classifier.py <ì´ë¯¸ì§€_ê²½ë¡œ>")
        print("ì˜ˆì‹œ: python3 test_classifier.py person_crops/helmet/test.jpg")
        sys.exit(1)
    
    image_path = sys.argv[1]
    result = predict_image(image_path)
    
    if result:
        label, confidence = result
        print(f"\nğŸ“¸ ì´ë¯¸ì§€: {image_path}")
        print(f"ğŸ¯ ì˜ˆì¸¡: {label}")
        print(f"ğŸ“Š ì‹ ë¢°ë„: {confidence:.2f}%")
EOF

chmod +x test_classifier.py
```

**2-6. í…ŒìŠ¤íŠ¸ ì‹¤í–‰**
```bash
# helmet í´ë”ì˜ ì´ë¯¸ì§€ë¡œ í…ŒìŠ¤íŠ¸
python3 test_classifier.py person_crops/helmet/S2-N6001M00001_person0.jpg

# no_helmet í´ë”ì˜ ì´ë¯¸ì§€ë¡œ í…ŒìŠ¤íŠ¸
python3 test_classifier.py person_crops/no_helmet/S2-N6002M00050_person1.jpg
```

**ì˜ˆìƒ ì¶œë ¥:**
```
ğŸ¤– ëª¨ë¸ ë¡œë”© ì¤‘...
âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ

ğŸ“¸ ì´ë¯¸ì§€: person_crops/helmet/S2-N6001M00001_person0.jpg
ğŸ¯ ì˜ˆì¸¡: helmet
ğŸ“Š ì‹ ë¢°ë„: 94.32%
```

---

## Step 3: íŒŒì´í”„ë¼ì¸ í†µí•©í•˜ê¸°

ì´ì œ í•™ìŠµí•œ ë¶„ë¥˜ ëª¨ë¸ì„ main_scale.pyì— í†µí•©í•©ë‹ˆë‹¤.

### ğŸ“ í†µí•© ê°€ì´ë“œ ë¬¸ì„œ ìƒì„±

```bash
cd /home/jera/Sentinel_AI_Project
cat > INTEGRATION_GUIDE.md << 'EOF'
# 2ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ í†µí•© ê°€ì´ë“œ

## í•„ìš”í•œ íŒŒì¼

1. í•™ìŠµëœ ëª¨ë¸: `helmet_classifier_best.h5`
2. ìˆ˜ì •í•  íŒŒì¼: `anu_example/main_scale.py`

## í†µí•© ë°©ë²•

### 1ë‹¨ê³„: ëª¨ë¸ íŒŒì¼ ë³µì‚¬

```bash
# í•™ìŠµí•œ ëª¨ë¸ì„ anu_exampleë¡œ ë³µì‚¬
cp two_stage_data/helmet_classifier_best.h5 anu_example/model/
```

### 2ë‹¨ê³„: ë¶„ë¥˜ê¸° í´ë˜ìŠ¤ ì¶”ê°€

`anu_example/main_scale.py` íŒŒì¼ ìƒë‹¨ì— ì¶”ê°€:

```python
import tensorflow as tf
from tensorflow import keras

class HelmetClassifier:
    """í—¬ë©§ ì°©ìš© ì—¬ë¶€ ë¶„ë¥˜ê¸°"""
    def __init__(self, model_path):
        self.model = keras.models.load_model(model_path)
        print(f"[HelmetClassifier] ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_path}")
    
    def predict(self, image):
        """
        í¬ë¡­ëœ ì‚¬ëŒ ì´ë¯¸ì§€ì—ì„œ í—¬ë©§ ì°©ìš© ì—¬ë¶€ ì˜ˆì¸¡
        
        Args:
            image: BGR ì´ë¯¸ì§€ (numpy array)
        
        Returns:
            (label, confidence): ('helmet' or 'no_helmet', ì‹ ë¢°ë„ 0-1)
        """
        # ì „ì²˜ë¦¬
        img_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        img_resized = cv2.resize(img_rgb, (224, 224))
        img_normalized = img_resized / 255.0
        img_batch = np.expand_dims(img_normalized, axis=0)
        
        # ì˜ˆì¸¡
        prediction = self.model.predict(img_batch, verbose=0)[0][0]
        
        # ê²°ê³¼
        if prediction < 0.5:
            return 'helmet', (1 - prediction)
        else:
            return 'no_helmet', prediction
```

### 3ë‹¨ê³„: DetectParser ìˆ˜ì •

```python
class DetectParser(multiprocessing.Process):
    def __init__(self, queue_in, queue_out, save_crops=True, use_two_stage=False):
        # ... ê¸°ì¡´ ì½”ë“œ ...
        self.use_two_stage = use_two_stage
        self.helmet_classifier = None
    
    def add_model(self):
        # Stage 1: Person Detector
        self.person_net = Yolo(
            'model/person5l/model.cfg',
            'model/person5l/model.weights',
            'model/person5l/model.names'
        )
        
        # Stage 2: Helmet Classifier (ì˜µì…˜)
        if self.use_two_stage:
            self.helmet_classifier = HelmetClassifier('model/helmet_classifier_best.h5')
    
    def run(self):
        self.add_model()
        frame_cnt = 0
        
        while True:
            frame_pair = self.queue_in.get()
            if frame_pair is None:
                self.queue_out.put(None)
                break
            
            orig_frame, up_frame = frame_pair
            frame_cnt += 1
            
            if frame_cnt % 3 == 0:
                # Stage 1: Detect persons
                yolo_dets_orig = self.person_net.detect(orig_frame, 0.5, 0.5)
                
                # Stage 2: Classify helmet (ì˜µì…˜)
                if self.use_two_stage and self.helmet_classifier:
                    for det in yolo_dets_orig:
                        if det['label'] == 0:  # person í´ë˜ìŠ¤ë§Œ
                            # í¬ë¡­
                            x1, y1, x2, y2 = det['bbox']
                            crop = orig_frame[y1:y2, x1:x2]
                            
                            if crop.size > 0:
                                # í—¬ë©§ ë¶„ë¥˜
                                helmet_label, confidence = self.helmet_classifier.predict(crop)
                                det['helmet_status'] = helmet_label
                                det['helmet_confidence'] = confidence
                
                # ì¶”ì 
                tracks_orig, _ = self.tracker_orig.update(yolo_dets_orig, orig_frame)
                
                # ... ë‚˜ë¨¸ì§€ ì½”ë“œ ...
```

### 4ë‹¨ê³„: main í•¨ìˆ˜ì— ì˜µì…˜ ì¶”ê°€

```python
if __name__ == '__main__':
    parser.add_argument('--two-stage', action='store_true',
                        help='2ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ í™œì„±í™”')
    
    # ...
    
    detector = DetectParser(q_video, q_detect, 
                           save_crops=args.save_crops,
                           use_two_stage=args.two_stage)
```

### 5ë‹¨ê³„: ì‹¤í–‰

```bash
# 2ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì‹¤í–‰
cd /home/jera/Sentinel_AI_Project/anu_example
source venv/bin/activate
python3 main_scale.py --two-stage
```

## ì„±ëŠ¥ ë¹„êµ

ì‹¤í–‰ í›„:
- Single-Stage (ê¸°ì¡´): ë¹ ë¥´ì§€ë§Œ ì˜¤íƒì§€ ê°€ëŠ¥
- Two-Stage (ê°œì„ ): ëŠë¦¬ì§€ë§Œ ì •í™•ë„ ë†’ìŒ

ë‘˜ ë‹¤ í…ŒìŠ¤íŠ¸í•´ë³´ê³  ì„ íƒí•˜ì„¸ìš”!
EOF
```

---

## ë¬¸ì œ í•´ê²°

### Q1: í¬ë¡­ ì´ë¯¸ì§€ê°€ ë„ˆë¬´ ì ì–´ìš”
**A:** `crop_persons.py`ì—ì„œ `min_size=50`ì„ `min_size=30`ìœ¼ë¡œ ì¤„ì—¬ë³´ì„¸ìš”.

### Q2: TensorFlow ì„¤ì¹˜ ì˜¤ë¥˜
**A:** 
```bash
pip install tensorflow-cpu==2.10.0  # CPU ë²„ì „ë§Œ
```

### Q3: í•™ìŠµì´ ë„ˆë¬´ ì˜¤ë˜ ê±¸ë ¤ìš”
**A:** `EPOCHS = 20`ì„ `EPOCHS = 10`ìœ¼ë¡œ ì¤„ì´ì„¸ìš”.

### Q4: ì •í™•ë„ê°€ ë„ˆë¬´ ë‚®ì•„ìš”
**A:** 
1. ë°ì´í„°ë¥¼ ë” ìˆ˜ì§‘í•˜ì„¸ìš” (ê° í´ë˜ìŠ¤ 200ê°œ ì´ìƒ)
2. ë¼ë²¨ë§ì„ ë‹¤ì‹œ í™•ì¸í•˜ì„¸ìš”
3. EPOCHSë¥¼ 30ìœ¼ë¡œ ëŠ˜ë ¤ë³´ì„¸ìš”

### Q5: ë©”ëª¨ë¦¬ ë¶€ì¡± ì˜¤ë¥˜
**A:** `BATCH_SIZE = 32`ë¥¼ `BATCH_SIZE = 16`ìœ¼ë¡œ ì¤„ì´ì„¸ìš”.

---

## ğŸ“š ì°¸ê³  ìë£Œ

- TensorFlow ê³µì‹ ë¬¸ì„œ: https://www.tensorflow.org/
- MobileNet ë…¼ë¬¸: https://arxiv.org/abs/1704.04861
- ì´ë¯¸ì§€ ë¶„ë¥˜ íŠœí† ë¦¬ì–¼: https://www.tensorflow.org/tutorials/images/classification

---

## âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸

ì™„ë£Œí•œ ë‹¨ê³„ë¥¼ ì²´í¬í•˜ì„¸ìš”:

**Step 1: ë°ì´í„° ì¤€ë¹„**
- [ ] crop_persons.py ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
- [ ] ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ìœ¼ë¡œ person í¬ë¡­
- [ ] helmet / no_helmet ë¼ë²¨ë§ ì™„ë£Œ
- [ ] ê° í´ë˜ìŠ¤ ìµœì†Œ 100ê°œ í™•ë³´

**Step 2: ëª¨ë¸ í•™ìŠµ**
- [ ] TensorFlow ì„¤ì¹˜
- [ ] train_helmet_classifier.py ìƒì„±
- [ ] ëª¨ë¸ í•™ìŠµ ì™„ë£Œ
- [ ] í•™ìŠµ ê·¸ë˜í”„ í™•ì¸
- [ ] í…ŒìŠ¤íŠ¸ë¡œ ê²€ì¦

**Step 3: í†µí•©**
- [ ] ëª¨ë¸ íŒŒì¼ ë³µì‚¬
- [ ] main_scale.py ìˆ˜ì •
- [ ] 2ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸
- [ ] ì„±ëŠ¥ ë¹„êµ

---

**ì‘ì„±ì¼**: 2024-10-14
**ëŒ€ìƒ**: ë¹„ì „ê³µì
**ì˜ˆìƒ ì†Œìš” ì‹œê°„**: 4-6ì‹œê°„
**ë‚œì´ë„**: â­â­â­â˜†â˜†

í™”ì´íŒ…! ì²œì²œíˆ ë”°ë¼í•˜ì‹œë©´ ë©ë‹ˆë‹¤. ë§‰íˆëŠ” ë¶€ë¶„ì´ ìˆìœ¼ë©´ ì–¸ì œë“  ì§ˆë¬¸í•˜ì„¸ìš”! ğŸš€
